{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faresffa/Champollion-/blob/main/Spark_Fares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercice 1 : Création et Manipulation de RDDs**\n"
      ],
      "metadata": {
        "id": "yjJBnXhd1eRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-isZ1_y11AAW",
        "outputId": "15258374-ac20-459f-9224-8e5a2761126a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombres pairs: [2, 4, 6, 8, 10]\n",
            "Carrés des nombres pairs: [4, 16, 36, 64, 100]\n",
            "Somme des carrés: 220\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"Exercice 1: Manipulation de RDDs\")\n",
        "\n",
        "# Création d'un RDD à partir d'une liste de nombres\n",
        "nombres = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd_nombres = sc.parallelize(nombres)\n",
        "\n",
        "# 1. Filtrer les nombres pairs\n",
        "rdd_pairs = rdd_nombres.filter(lambda x: x % 2 == 0)\n",
        "print(\"Nombres pairs:\", rdd_pairs.collect())\n",
        "\n",
        "# 2. Calculer le carré de chaque nombre filtré\n",
        "rdd_carres = rdd_pairs.map(lambda x: x ** 2)\n",
        "print(\"Carrés des nombres pairs:\", rdd_carres.collect())\n",
        "\n",
        "# 3. Calculer la somme des nombres obtenus\n",
        "somme = rdd_carres.reduce(lambda x, y: x + y)\n",
        "print(\"Somme des carrés:\", somme)\n",
        "\n",
        "# Fermeture du contexte Spark\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercice 2 : Opérations de Base sur les DataFrames**"
      ],
      "metadata": {
        "id": "YldQ1DWw2qgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 2: DataFrames\").getOrCreate()\n",
        "\n",
        "# Création d'un DataFrame à partir d'une liste de tuples\n",
        "employes = [\n",
        "    (\"Alice\", \"RH\", 55000),\n",
        "    (\"Bob\", \"Informatique\", 65000),\n",
        "    (\"Charlie\", \"Ventes\", 48000),\n",
        "    (\"David\", \"Informatique\", 70000),\n",
        "    (\"Eve\", \"RH\", 52000),\n",
        "    (\"Frank\", \"Ventes\", 49000)\n",
        "]\n",
        "\n",
        "# Création du DataFrame avec les noms de colonnes\n",
        "df_employes = spark.createDataFrame(employes, [\"Nom\", \"Departement\", \"Salaire\"])\n",
        "\n",
        "# 1. Afficher le schéma du DataFrame\n",
        "print(\"Schéma du DataFrame:\")\n",
        "df_employes.printSchema()\n",
        "\n",
        "# 2. Afficher les données\n",
        "print(\"Données du DataFrame:\")\n",
        "df_employes.show()\n",
        "\n",
        "# 3. Filtrer les employés dont le salaire est supérieur à 50000\n",
        "print(\"Employés avec un salaire > 50000:\")\n",
        "df_employes.filter(df_employes.Salaire > 50000).show()\n",
        "\n",
        "# 4. Calculer le salaire moyen par département\n",
        "print(\"Salaire moyen par département:\")\n",
        "df_employes.groupBy(\"Departement\").agg(avg(\"Salaire\").alias(\"Salaire_Moyen\")).show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV72b44w1-sQ",
        "outputId": "bd2d7a04-13f2-4a6b-f362-2d80ce91c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schéma du DataFrame:\n",
            "root\n",
            " |-- Nom: string (nullable = true)\n",
            " |-- Departement: string (nullable = true)\n",
            " |-- Salaire: long (nullable = true)\n",
            "\n",
            "Données du DataFrame:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|  55000|\n",
            "|    Bob|Informatique|  65000|\n",
            "|Charlie|      Ventes|  48000|\n",
            "|  David|Informatique|  70000|\n",
            "|    Eve|          RH|  52000|\n",
            "|  Frank|      Ventes|  49000|\n",
            "+-------+------------+-------+\n",
            "\n",
            "Employés avec un salaire > 50000:\n",
            "+-----+------------+-------+\n",
            "|  Nom| Departement|Salaire|\n",
            "+-----+------------+-------+\n",
            "|Alice|          RH|  55000|\n",
            "|  Bob|Informatique|  65000|\n",
            "|David|Informatique|  70000|\n",
            "|  Eve|          RH|  52000|\n",
            "+-----+------------+-------+\n",
            "\n",
            "Salaire moyen par département:\n",
            "+------------+-------------+\n",
            "| Departement|Salaire_Moyen|\n",
            "+------------+-------------+\n",
            "|Informatique|      67500.0|\n",
            "|      Ventes|      48500.0|\n",
            "|          RH|      53500.0|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercice 3 : Chargement de Données à Partir d'un Fichier CSV**"
      ],
      "metadata": {
        "id": "jRHtWzWF3Ynf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Nom du fichier CSV à créer\n",
        "nom_fichier = \"produits.csv\"\n",
        "\n",
        "# Données à écrire dans le fichier CSV\n",
        "donnees = [\n",
        "    [\"ID\", \"Nom\", \"Categorie\", \"Prix\"],  # En-têtes\n",
        "    [1, \"iPhone\", \"Electronique\", 999.99],\n",
        "    [2, \"Galaxy S22\", \"Electronique\", 899.99],\n",
        "    [3, \"MacBook Pro\", \"Electronique\", 1999.99],\n",
        "    [4, \"T-shirt\", \"Vetements\", 19.99],\n",
        "    [5, \"Jeans\", \"Vetements\", 59.99],\n",
        "    [6, \"Chaussures\", \"Chaussures\", 89.99],\n",
        "    [7, \"Ballon de football\", \"Sport\", 29.99],\n",
        "    [8, \"Raquette de tennis\", \"Sport\", 149.99]\n",
        "]\n",
        "\n",
        "# Création du fichier CSV\n",
        "with open(nom_fichier, mode=\"w\", newline=\"\", encoding=\"utf-8\") as fichier:\n",
        "    writer = csv.writer(fichier)\n",
        "    writer.writerows(donnees)\n",
        "\n",
        "print(f\"Le fichier {nom_fichier} a été créé avec succès !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9u-iL-D5z5q",
        "outputId": "5499b2aa-9b0c-4e57-9539-576c70d5c879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier produits.csv a été créé avec succès !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 3: CSV\").getOrCreate()\n",
        "\n",
        "# Supposons que nous avons un fichier produits.csv avec cette structure:\n",
        "# ID,Nom,Categorie,Prix\n",
        "# 1,iPhone,Electronique,999.99\n",
        "# 2,Galaxy S22,Electronique,899.99\n",
        "# 3,T-shirt,Vetements,19.99\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "# Création du DataFrame\n",
        "#df_produits = spark.createDataFrame(produits_data, [\"ID\", \"Nom\", \"Categorie\", \"Prix\"])\n",
        "\n",
        "# Commentez les lignes ci-dessus et décommentez la ligne ci-dessous pour charger un vrai fichier CSV\n",
        "df_produits = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"produits.csv\")\n",
        "\n",
        "# 1. Afficher les 5 premières lignes\n",
        "print(\"Les 5 premières lignes:\")\n",
        "df_produits.show(5)\n",
        "\n",
        "# 2. Filtrer les produits d'une catégorie spécifique (Electronique)\n",
        "print(\"Produits électroniques:\")\n",
        "df_produits.filter(df_produits.Categorie == \"Electronique\").show()\n",
        "\n",
        "# 3. Calculer le prix moyen des produits par catégorie\n",
        "print(\"Prix moyen par catégorie:\")\n",
        "df_produits.groupBy(\"Categorie\").agg(avg(\"Prix\").alias(\"Prix_Moyen\")).show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjhXGpRD3qqX",
        "outputId": "4480e480-8a8b-4b3b-a565-e184ed2c5a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les 5 premières lignes:\n",
            "+---+-----------+------------+-------+\n",
            "| ID|        Nom|   Categorie|   Prix|\n",
            "+---+-----------+------------+-------+\n",
            "|  1|     iPhone|Electronique| 999.99|\n",
            "|  2| Galaxy S22|Electronique| 899.99|\n",
            "|  3|MacBook Pro|Electronique|1999.99|\n",
            "|  4|    T-shirt|   Vetements|  19.99|\n",
            "|  5|      Jeans|   Vetements|  59.99|\n",
            "+---+-----------+------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Produits électroniques:\n",
            "+---+-----------+------------+-------+\n",
            "| ID|        Nom|   Categorie|   Prix|\n",
            "+---+-----------+------------+-------+\n",
            "|  1|     iPhone|Electronique| 999.99|\n",
            "|  2| Galaxy S22|Electronique| 899.99|\n",
            "|  3|MacBook Pro|Electronique|1999.99|\n",
            "+---+-----------+------------+-------+\n",
            "\n",
            "Prix moyen par catégorie:\n",
            "+------------+-----------------+\n",
            "|   Categorie|       Prix_Moyen|\n",
            "+------------+-----------------+\n",
            "|   Vetements|            39.99|\n",
            "|  Chaussures|            89.99|\n",
            "|       Sport|89.99000000000001|\n",
            "|Electronique|          1299.99|\n",
            "+------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercice 4 : Utilisation de Fonctions UDF (User Defined Functions) texte en gras**"
      ],
      "metadata": {
        "id": "EQtSWqIB7Cj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 4: UDF\").getOrCreate()\n",
        "\n",
        "# Chargement des données depuis le fichier CSV\n",
        "df_produits = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"produits.csv\")\n",
        "\n",
        "# Vérification du schéma pour s'assurer que \"Prix\" est bien un type numérique\n",
        "df_produits.printSchema()\n",
        "\n",
        "# Définition de la fonction UDF pour catégoriser les prix\n",
        "def categorie_prix(prix, seuil=100):\n",
        "    if prix >= seuil:\n",
        "        return \"Élevé\"\n",
        "    else:\n",
        "        return \"Bas\"\n",
        "\n",
        "# Enregistrement de la fonction UDF\n",
        "categorie_prix_udf = udf(categorie_prix, StringType())\n",
        "\n",
        "# Application de l'UDF au DataFrame\n",
        "df_produits_avec_categorie = df_produits.withColumn(\"Categorie_Prix\", categorie_prix_udf(df_produits[\"Prix\"]))\n",
        "\n",
        "# Affichage du résultat\n",
        "print(\"Produits avec catégorie de prix:\")\n",
        "df_produits_avec_categorie.show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAAmcRK8665I",
        "outputId": "bdff50b3-69f7-4988-fd24-d0a5d6944be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Nom: string (nullable = true)\n",
            " |-- Categorie: string (nullable = true)\n",
            " |-- Prix: double (nullable = true)\n",
            "\n",
            "Produits avec catégorie de prix:\n",
            "+---+------------------+------------+-------+--------------+\n",
            "| ID|               Nom|   Categorie|   Prix|Categorie_Prix|\n",
            "+---+------------------+------------+-------+--------------+\n",
            "|  1|            iPhone|Electronique| 999.99|         Élevé|\n",
            "|  2|        Galaxy S22|Electronique| 899.99|         Élevé|\n",
            "|  3|       MacBook Pro|Electronique|1999.99|         Élevé|\n",
            "|  4|           T-shirt|   Vetements|  19.99|           Bas|\n",
            "|  5|             Jeans|   Vetements|  59.99|           Bas|\n",
            "|  6|        Chaussures|  Chaussures|  89.99|           Bas|\n",
            "|  7|Ballon de football|       Sport|  29.99|           Bas|\n",
            "|  8|Raquette de tennis|       Sport| 149.99|         Élevé|\n",
            "+---+------------------+------------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercice 5 : Jointure de Deux DataFrames**"
      ],
      "metadata": {
        "id": "XcmUIRqH837p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 5: Jointures\").getOrCreate()\n",
        "\n",
        "# Création du DataFrame des commandes\n",
        "commandes_data = [\n",
        "    (1, 101, 150.0),\n",
        "    (2, 102, 200.5),\n",
        "    (3, 101, 300.0),\n",
        "    (4, 103, 550.75),\n",
        "    (5, 104, 170.25),\n",
        "    (6, 102, 125.0)\n",
        "]\n",
        "df_commandes = spark.createDataFrame(commandes_data, [\"ID_Commande\", \"ID_Client\", \"Montant\"])\n",
        "\n",
        "# Création du DataFrame des clients\n",
        "clients_data = [\n",
        "    (101, \"Alice\", \"France\"),\n",
        "    (102, \"Bob\", \"Belgique\"),\n",
        "    (103, \"Charlie\", \"Suisse\"),\n",
        "    (104, \"David\", \"France\"),\n",
        "    (105, \"Eve\", \"Allemagne\")\n",
        "]\n",
        "df_clients = spark.createDataFrame(clients_data, [\"ID_Client\", \"Nom\", \"Pays\"])\n",
        "\n",
        "# Affichage des DataFrames sources\n",
        "print(\"DataFrame des commandes:\")\n",
        "df_commandes.show()\n",
        "\n",
        "print(\"DataFrame des clients:\")\n",
        "df_clients.show()\n",
        "\n",
        "# Jointure des deux DataFrames sur ID_Client\n",
        "df_commandes_clients = df_commandes.join(df_clients, \"ID_Client\", \"inner\")\n",
        "\n",
        "# Affichage du résultat de la jointure\n",
        "print(\"Résultat de la jointure:\")\n",
        "df_commandes_clients.show()\n",
        "\n",
        "# Sélection des colonnes pertinentes\n",
        "df_commandes_clients_select = df_commandes_clients.select(\"ID_Commande\", \"Nom\", \"Pays\", \"Montant\")\n",
        "\n",
        "# Affichage du résultat final\n",
        "print(\"Résultat final avec les colonnes sélectionnées:\")\n",
        "df_commandes_clients_select.show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP0IVCdq9F_Y",
        "outputId": "e749d352-4a7b-4a68-ca04-470c00075b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame des commandes:\n",
            "+-----------+---------+-------+\n",
            "|ID_Commande|ID_Client|Montant|\n",
            "+-----------+---------+-------+\n",
            "|          1|      101|  150.0|\n",
            "|          2|      102|  200.5|\n",
            "|          3|      101|  300.0|\n",
            "|          4|      103| 550.75|\n",
            "|          5|      104| 170.25|\n",
            "|          6|      102|  125.0|\n",
            "+-----------+---------+-------+\n",
            "\n",
            "DataFrame des clients:\n",
            "+---------+-------+---------+\n",
            "|ID_Client|    Nom|     Pays|\n",
            "+---------+-------+---------+\n",
            "|      101|  Alice|   France|\n",
            "|      102|    Bob| Belgique|\n",
            "|      103|Charlie|   Suisse|\n",
            "|      104|  David|   France|\n",
            "|      105|    Eve|Allemagne|\n",
            "+---------+-------+---------+\n",
            "\n",
            "Résultat de la jointure:\n",
            "+---------+-----------+-------+-------+--------+\n",
            "|ID_Client|ID_Commande|Montant|    Nom|    Pays|\n",
            "+---------+-----------+-------+-------+--------+\n",
            "|      101|          1|  150.0|  Alice|  France|\n",
            "|      101|          3|  300.0|  Alice|  France|\n",
            "|      102|          2|  200.5|    Bob|Belgique|\n",
            "|      102|          6|  125.0|    Bob|Belgique|\n",
            "|      103|          4| 550.75|Charlie|  Suisse|\n",
            "|      104|          5| 170.25|  David|  France|\n",
            "+---------+-----------+-------+-------+--------+\n",
            "\n",
            "Résultat final avec les colonnes sélectionnées:\n",
            "+-----------+-------+--------+-------+\n",
            "|ID_Commande|    Nom|    Pays|Montant|\n",
            "+-----------+-------+--------+-------+\n",
            "|          1|  Alice|  France|  150.0|\n",
            "|          3|  Alice|  France|  300.0|\n",
            "|          2|    Bob|Belgique|  200.5|\n",
            "|          6|    Bob|Belgique|  125.0|\n",
            "|          4|Charlie|  Suisse| 550.75|\n",
            "|          5|  David|  France| 170.25|\n",
            "+-----------+-------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Exercice 6 : Agrégation et Groupement***"
      ],
      "metadata": {
        "id": "oSaOCUsoifpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 6: Agrégation\").getOrCreate()\n",
        "\n",
        "# Utilisation des mêmes DataFrames que l'exercice 5\n",
        "commandes_data = [\n",
        "    (1, 101, 150.0),\n",
        "    (2, 102, 200.5),\n",
        "    (3, 101, 300.0),\n",
        "    (4, 103, 550.75),\n",
        "    (5, 104, 170.25),\n",
        "    (6, 102, 125.0)\n",
        "]\n",
        "df_commandes = spark.createDataFrame(commandes_data, [\"ID_Commande\", \"ID_Client\", \"Montant\"])\n",
        "\n",
        "clients_data = [\n",
        "    (101, \"Alice\", \"France\"),\n",
        "    (102, \"Bob\", \"Belgique\"),\n",
        "    (103, \"Charlie\", \"Suisse\"),\n",
        "    (104, \"David\", \"France\"),\n",
        "    (105, \"Eve\", \"Allemagne\")\n",
        "]\n",
        "df_clients = spark.createDataFrame(clients_data, [\"ID_Client\", \"Nom\", \"Pays\"])\n",
        "\n",
        "# Jointure des DataFrames\n",
        "df_joint = df_commandes.join(df_clients, \"ID_Client\", \"inner\")\n",
        "\n",
        "# Calcul du montant total des commandes par pays\n",
        "df_montant_par_pays = df_joint.groupBy(\"Pays\").agg(spark_sum(\"Montant\").alias(\"Montant_Total\"))\n",
        "\n",
        "# Tri par montant décroissant\n",
        "df_montant_tri = df_montant_par_pays.orderBy(desc(\"Montant_Total\"))\n",
        "\n",
        "# Affichage du résultat\n",
        "print(\"Montant total des commandes par pays (trié par montant décroissant):\")\n",
        "df_montant_tri.show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMDn_tsEilc5",
        "outputId": "5caa7b21-a661-4d9b-bae9-d425c148078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montant total des commandes par pays (trié par montant décroissant):\n",
            "+--------+-------------+\n",
            "|    Pays|Montant_Total|\n",
            "+--------+-------------+\n",
            "|  France|       620.25|\n",
            "|  Suisse|       550.75|\n",
            "|Belgique|        325.5|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Exercice 7 : Gestion des Valeurs Manquantes***"
      ],
      "metadata": {
        "id": "JJ1Y94cmjUuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, col, when\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 7: Valeurs Manquantes\").getOrCreate()\n",
        "\n",
        "# Création d'un DataFrame avec des valeurs manquantes\n",
        "employes_data = [\n",
        "    (\"Alice\", \"RH\", 55000),\n",
        "    (\"Bob\", \"Informatique\", None),\n",
        "    (\"Charlie\", \"Ventes\", 48000),\n",
        "    (\"David\", \"Informatique\", 70000),\n",
        "    (\"Eve\", None, 52000),\n",
        "    (\"Frank\", \"Ventes\", None)\n",
        "]\n",
        "df_employes = spark.createDataFrame(employes_data, [\"Nom\", \"Departement\", \"Salaire\"])\n",
        "\n",
        "# Affichage du DataFrame original\n",
        "print(\"DataFrame original avec valeurs manquantes:\")\n",
        "df_employes.show()\n",
        "\n",
        "# Comptage des valeurs manquantes par colonne\n",
        "print(\"Nombre de valeurs manquantes par colonne:\")\n",
        "for col_name in df_employes.columns:\n",
        "    null_count = df_employes.filter(col(col_name).isNull()).count()\n",
        "    print(f\"Colonne {col_name}: {null_count} valeurs manquantes\")\n",
        "\n",
        "# Méthode 1: Suppression des lignes avec des valeurs manquantes\n",
        "df_sans_nulls = df_employes.dropna()\n",
        "print(\"\\nMéthode 1: Suppression des lignes avec des valeurs manquantes:\")\n",
        "df_sans_nulls.show()\n",
        "\n",
        "# Méthode 2: Remplacement des valeurs manquantes par des constantes\n",
        "df_rempli_constantes = df_employes.fillna({\n",
        "    \"Departement\": \"Non affecté\",\n",
        "    \"Salaire\": 0\n",
        "})\n",
        "print(\"\\nMéthode 2: Remplacement par des constantes:\")\n",
        "df_rempli_constantes.show()\n",
        "\n",
        "# Méthode 3: Remplacement des salaires manquants par la moyenne des salaires\n",
        "# D'abord, calculer la moyenne des salaires non-nuls\n",
        "salaire_moyen = df_employes.filter(col(\"Salaire\").isNotNull()).select(avg(\"Salaire\")).collect()[0][0]\n",
        "print(f\"\\nSalaire moyen: {salaire_moyen}\")\n",
        "\n",
        "# Ensuite, remplacer les salaires manquants par cette moyenne\n",
        "df_rempli_moyenne = df_employes.fillna({\n",
        "    \"Departement\": \"Non affecté\"\n",
        "}).withColumn(\n",
        "    \"Salaire\",\n",
        "    when(col(\"Salaire\").isNull(), salaire_moyen).otherwise(col(\"Salaire\"))\n",
        ")\n",
        "print(\"\\nMéthode 3: Remplacement des salaires par la moyenne:\")\n",
        "df_rempli_moyenne.show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saSAysGkjpzm",
        "outputId": "6ede835a-8ed8-48e4-99e2-737db73c9ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame original avec valeurs manquantes:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|  55000|\n",
            "|    Bob|Informatique|   NULL|\n",
            "|Charlie|      Ventes|  48000|\n",
            "|  David|Informatique|  70000|\n",
            "|    Eve|        NULL|  52000|\n",
            "|  Frank|      Ventes|   NULL|\n",
            "+-------+------------+-------+\n",
            "\n",
            "Nombre de valeurs manquantes par colonne:\n",
            "Colonne Nom: 0 valeurs manquantes\n",
            "Colonne Departement: 1 valeurs manquantes\n",
            "Colonne Salaire: 2 valeurs manquantes\n",
            "\n",
            "Méthode 1: Suppression des lignes avec des valeurs manquantes:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|  55000|\n",
            "|Charlie|      Ventes|  48000|\n",
            "|  David|Informatique|  70000|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Méthode 2: Remplacement par des constantes:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|  55000|\n",
            "|    Bob|Informatique|      0|\n",
            "|Charlie|      Ventes|  48000|\n",
            "|  David|Informatique|  70000|\n",
            "|    Eve| Non affecté|  52000|\n",
            "|  Frank|      Ventes|      0|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Salaire moyen: 56250.0\n",
            "\n",
            "Méthode 3: Remplacement des salaires par la moyenne:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|55000.0|\n",
            "|    Bob|Informatique|56250.0|\n",
            "|Charlie|      Ventes|48000.0|\n",
            "|  David|Informatique|70000.0|\n",
            "|    Eve| Non affecté|52000.0|\n",
            "|  Frank|      Ventes|56250.0|\n",
            "+-------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Exercice 8 : Partitionnement et Optimisation***"
      ],
      "metadata": {
        "id": "D3j9_JeakZ-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 8: Partitionnement\").getOrCreate()\n",
        "\n",
        "# Génération de données pour l'exemple\n",
        "from pyspark.sql.functions import lit, expr\n",
        "import string\n",
        "import random\n",
        "\n",
        "# Fonction pour générer des données aléatoires\n",
        "def generate_random_data(n):\n",
        "    return [(\n",
        "        i,\n",
        "        ''.join(random.choices(string.ascii_letters, k=10)),\n",
        "        random.randint(18, 65),\n",
        "        random.choice([\"A\", \"B\", \"C\", \"D\", \"E\"]),\n",
        "        random.uniform(1000, 10000)\n",
        "    ) for i in range(n)]\n",
        "\n",
        "# Génération de 100 000 lignes de données ()\n",
        "data_size = 100000\n",
        "data = generate_random_data(data_size)\n",
        "df = spark.createDataFrame(data, [\"ID\", \"Nom\", \"Age\", \"Categorie\", \"Valeur\"])\n",
        "\n",
        "# Affichage d'un échantillon du DataFrame\n",
        "print(\"Échantillon du DataFrame:\")\n",
        "df.show(5)\n",
        "\n",
        "# Vérification du nombre de partitions par défaut\n",
        "num_partitions_default = df.rdd.getNumPartitions()\n",
        "print(f\"Nombre de partitions par défaut: {num_partitions_default}\")\n",
        "\n",
        "# Benchmark avec le nombre de partitions par défaut\n",
        "start_time = time.time()\n",
        "df.groupBy(\"Categorie\").count().show()\n",
        "default_time = time.time() - start_time\n",
        "print(f\"Temps d'exécution avec {num_partitions_default} partitions: {default_time:.2f} secondes\")\n",
        "\n",
        "# Repartitionnement du DataFrame avec un nombre différent de partitions\n",
        "num_partitions_new = 8  # Ajustez selon votre machine\n",
        "df_repartitionne = df.repartition(num_partitions_new)\n",
        "\n",
        "# Benchmark avec le nouveau nombre de partitions\n",
        "start_time = time.time()\n",
        "df_repartitionne.groupBy(\"Categorie\").count().show()\n",
        "new_time = time.time() - start_time\n",
        "print(f\"Temps d'exécution avec {num_partitions_new} partitions: {new_time:.2f} secondes\")\n",
        "\n",
        "# Optimisation avec le partitionnement par clé (ici, Categorie)\n",
        "df_partitionne_par_cle = df.repartition(\"Categorie\")\n",
        "\n",
        "# Benchmark avec partitionnement par clé\n",
        "start_time = time.time()\n",
        "df_partitionne_par_cle.groupBy(\"Categorie\").count().show()\n",
        "key_time = time.time() - start_time\n",
        "print(f\"Temps d'exécution avec partitionnement par clé: {key_time:.2f} secondes\")\n",
        "\n",
        "# Mise en cache pour améliorer les performances\n",
        "df_cache = df.cache()\n",
        "\n",
        "# Benchmark avec mise en cache\n",
        "start_time = time.time()\n",
        "df_cache.groupBy(\"Categorie\").count().show()\n",
        "cache_time = time.time() - start_time\n",
        "print(f\"Temps d'exécution avec mise en cache: {cache_time:.2f} secondes\")\n",
        "\n",
        "# Deuxième exécution sur le DataFrame en cache\n",
        "start_time = time.time()\n",
        "df_cache.groupBy(\"Categorie\").count().show()\n",
        "cache_time2 = time.time() - start_time\n",
        "print(f\"Temps d'exécution pour la deuxième exécution avec cache: {cache_time2:.2f} secondes\")\n",
        "\n",
        "# Comparaison des performances\n",
        "print(\"\\nComparaison des performances:\")\n",
        "print(f\"Partitions par défaut ({num_partitions_default}): {default_time:.2f} secondes\")\n",
        "print(f\"Repartitionnement ({num_partitions_new}): {new_time:.2f} secondes\")\n",
        "print(f\"Partitionnement par clé: {key_time:.2f} secondes\")\n",
        "print(f\"Mise en cache (première exécution): {cache_time:.2f} secondes\")\n",
        "print(f\"Mise en cache (deuxième exécution): {cache_time2:.2f} secondes\")\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ7AhNXlkfox",
        "outputId": "d0b649d0-ef88-4bd4-d7dc-f51691560432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Échantillon du DataFrame:\n",
            "+---+----------+---+---------+------------------+\n",
            "| ID|       Nom|Age|Categorie|            Valeur|\n",
            "+---+----------+---+---------+------------------+\n",
            "|  0|XXOcWqUkjR| 32|        A| 5156.828501116695|\n",
            "|  1|bQDbSJOgBY| 40|        C| 5202.276294863273|\n",
            "|  2|PGxcGrYnKd| 30|        D| 7573.119742411222|\n",
            "|  3|SIJnjCaxdc| 36|        B|1090.9879207287124|\n",
            "|  4|kmYmhlmBcx| 39|        E| 5802.764012410424|\n",
            "+---+----------+---+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Nombre de partitions par défaut: 2\n",
            "+---------+-----+\n",
            "|Categorie|count|\n",
            "+---------+-----+\n",
            "|        E|19899|\n",
            "|        B|20065|\n",
            "|        D|20135|\n",
            "|        C|19893|\n",
            "|        A|20008|\n",
            "+---------+-----+\n",
            "\n",
            "Temps d'exécution avec 2 partitions: 1.15 secondes\n",
            "+---------+-----+\n",
            "|Categorie|count|\n",
            "+---------+-----+\n",
            "|        E|19899|\n",
            "|        B|20065|\n",
            "|        D|20135|\n",
            "|        C|19893|\n",
            "|        A|20008|\n",
            "+---------+-----+\n",
            "\n",
            "Temps d'exécution avec 8 partitions: 1.97 secondes\n",
            "+---------+-----+\n",
            "|Categorie|count|\n",
            "+---------+-----+\n",
            "|        E|19899|\n",
            "|        B|20065|\n",
            "|        D|20135|\n",
            "|        C|19893|\n",
            "|        A|20008|\n",
            "+---------+-----+\n",
            "\n",
            "Temps d'exécution avec partitionnement par clé: 1.55 secondes\n",
            "+---------+-----+\n",
            "|Categorie|count|\n",
            "+---------+-----+\n",
            "|        E|19899|\n",
            "|        B|20065|\n",
            "|        D|20135|\n",
            "|        C|19893|\n",
            "|        A|20008|\n",
            "+---------+-----+\n",
            "\n",
            "Temps d'exécution avec mise en cache: 1.92 secondes\n",
            "+---------+-----+\n",
            "|Categorie|count|\n",
            "+---------+-----+\n",
            "|        E|19899|\n",
            "|        B|20065|\n",
            "|        D|20135|\n",
            "|        C|19893|\n",
            "|        A|20008|\n",
            "+---------+-----+\n",
            "\n",
            "Temps d'exécution pour la deuxième exécution avec cache: 0.49 secondes\n",
            "\n",
            "Comparaison des performances:\n",
            "Partitions par défaut (2): 1.15 secondes\n",
            "Repartitionnement (8): 1.97 secondes\n",
            "Partitionnement par clé: 1.55 secondes\n",
            "Mise en cache (première exécution): 1.92 secondes\n",
            "Mise en cache (deuxième exécution): 0.49 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 9 : Utilisation de Spark SQL***"
      ],
      "metadata": {
        "id": "6Pj0PRL6rYz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 9: Spark SQL\").getOrCreate()\n",
        "\n",
        "# Création de DataFrames pour l'exemple\n",
        "produits_data = [\n",
        "    (1, \"iPhone\", \"Electronique\", 999.99),\n",
        "    (2, \"Galaxy S22\", \"Electronique\", 899.99),\n",
        "    (3, \"MacBook Pro\", \"Electronique\", 1999.99),\n",
        "    (4, \"T-shirt\", \"Vetements\", 19.99),\n",
        "    (5, \"Jeans\", \"Vetements\", 59.99),\n",
        "    (6, \"Chaussures\", \"Chaussures\", 89.99),\n",
        "    (7, \"Ballon de football\", \"Sport\", 29.99),\n",
        "    (8, \"Raquette de tennis\", \"Sport\", 149.99)\n",
        "]\n",
        "\n",
        "ventes_data = [\n",
        "    (101, 1, 2, \"2023-01-15\"),\n",
        "    (102, 3, 1, \"2023-01-16\"),\n",
        "    (103, 5, 3, \"2023-01-17\"),\n",
        "    (104, 2, 1, \"2023-01-18\"),\n",
        "    (105, 4, 5, \"2023-01-19\"),\n",
        "    (106, 6, 2, \"2023-01-20\"),\n",
        "    (107, 7, 1, \"2023-01-21\"),\n",
        "    (108, 8, 1, \"2023-01-22\"),\n",
        "    (109, 1, 1, \"2023-01-23\")\n",
        "]\n",
        "\n",
        "# Création des DataFrames\n",
        "df_produits = spark.createDataFrame(produits_data, [\"ID\", \"Nom\", \"Categorie\", \"Prix\"])\n",
        "df_ventes = spark.createDataFrame(ventes_data, [\"ID_Vente\", \"ID_Produit\", \"Quantite\", \"Date\"])\n",
        "\n",
        "# Affichage des DataFrames\n",
        "print(\"DataFrame des produits:\")\n",
        "df_produits.show()\n",
        "\n",
        "print(\"DataFrame des ventes:\")\n",
        "df_ventes.show()\n",
        "\n",
        "# Enregistrement des DataFrames comme tables temporaires\n",
        "df_produits.createOrReplaceTempView(\"produits\")\n",
        "df_ventes.createOrReplaceTempView(\"ventes\")\n",
        "\n",
        "# Requête 1: Liste des produits avec leur prix\n",
        "print(\"Requête 1: Liste des produits avec leur prix:\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT ID, Nom, Prix\n",
        "FROM produits\n",
        "ORDER BY Prix DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Requête 2: Total des ventes par catégorie de produit\n",
        "print(\"Requête 2: Total des ventes par catégorie de produit:\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT p.Categorie, SUM(p.Prix * v.Quantite) as Total_Ventes\n",
        "FROM produits p\n",
        "JOIN ventes v ON p.ID = v.ID_Produit\n",
        "GROUP BY p.Categorie\n",
        "ORDER BY Total_Ventes DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Requête 3: Produits les plus vendus\n",
        "print(\"Requête 3: Produits les plus vendus:\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT p.Nom, SUM(v.Quantite) as Total_Quantite\n",
        "FROM produits p\n",
        "JOIN ventes v ON p.ID = v.ID_Produit\n",
        "GROUP BY p.Nom\n",
        "ORDER BY Total_Quantite DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# Requête 4: Ventes quotidiennes\n",
        "print(\"Requête 4: Ventes quotidiennes:\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT v.Date, COUNT(*) as Nombre_Ventes, SUM(p.Prix * v.Quantite) as Total_Ventes\n",
        "FROM ventes v\n",
        "JOIN produits p ON v.ID_Produit = p.ID\n",
        "GROUP BY v.Date\n",
        "ORDER BY v.Date\n",
        "\"\"\").show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioA9vNbOrdMb",
        "outputId": "dc0c2ae5-c1b1-4c9a-fe38-194b9b5cda29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame des produits:\n",
            "+---+------------------+------------+-------+\n",
            "| ID|               Nom|   Categorie|   Prix|\n",
            "+---+------------------+------------+-------+\n",
            "|  1|            iPhone|Electronique| 999.99|\n",
            "|  2|        Galaxy S22|Electronique| 899.99|\n",
            "|  3|       MacBook Pro|Electronique|1999.99|\n",
            "|  4|           T-shirt|   Vetements|  19.99|\n",
            "|  5|             Jeans|   Vetements|  59.99|\n",
            "|  6|        Chaussures|  Chaussures|  89.99|\n",
            "|  7|Ballon de football|       Sport|  29.99|\n",
            "|  8|Raquette de tennis|       Sport| 149.99|\n",
            "+---+------------------+------------+-------+\n",
            "\n",
            "DataFrame des ventes:\n",
            "+--------+----------+--------+----------+\n",
            "|ID_Vente|ID_Produit|Quantite|      Date|\n",
            "+--------+----------+--------+----------+\n",
            "|     101|         1|       2|2023-01-15|\n",
            "|     102|         3|       1|2023-01-16|\n",
            "|     103|         5|       3|2023-01-17|\n",
            "|     104|         2|       1|2023-01-18|\n",
            "|     105|         4|       5|2023-01-19|\n",
            "|     106|         6|       2|2023-01-20|\n",
            "|     107|         7|       1|2023-01-21|\n",
            "|     108|         8|       1|2023-01-22|\n",
            "|     109|         1|       1|2023-01-23|\n",
            "+--------+----------+--------+----------+\n",
            "\n",
            "Requête 1: Liste des produits avec leur prix:\n",
            "+---+------------------+-------+\n",
            "| ID|               Nom|   Prix|\n",
            "+---+------------------+-------+\n",
            "|  3|       MacBook Pro|1999.99|\n",
            "|  1|            iPhone| 999.99|\n",
            "|  2|        Galaxy S22| 899.99|\n",
            "|  8|Raquette de tennis| 149.99|\n",
            "|  6|        Chaussures|  89.99|\n",
            "|  5|             Jeans|  59.99|\n",
            "|  7|Ballon de football|  29.99|\n",
            "|  4|           T-shirt|  19.99|\n",
            "+---+------------------+-------+\n",
            "\n",
            "Requête 2: Total des ventes par catégorie de produit:\n",
            "+------------+------------------+\n",
            "|   Categorie|      Total_Ventes|\n",
            "+------------+------------------+\n",
            "|Electronique|           5899.95|\n",
            "|   Vetements|279.91999999999996|\n",
            "|       Sport|179.98000000000002|\n",
            "|  Chaussures|            179.98|\n",
            "+------------+------------------+\n",
            "\n",
            "Requête 3: Produits les plus vendus:\n",
            "+------------------+--------------+\n",
            "|               Nom|Total_Quantite|\n",
            "+------------------+--------------+\n",
            "|           T-shirt|             5|\n",
            "|            iPhone|             3|\n",
            "|             Jeans|             3|\n",
            "|        Chaussures|             2|\n",
            "|Ballon de football|             1|\n",
            "|Raquette de tennis|             1|\n",
            "|       MacBook Pro|             1|\n",
            "|        Galaxy S22|             1|\n",
            "+------------------+--------------+\n",
            "\n",
            "Requête 4: Ventes quotidiennes:\n",
            "+----------+-------------+-----------------+\n",
            "|      Date|Nombre_Ventes|     Total_Ventes|\n",
            "+----------+-------------+-----------------+\n",
            "|2023-01-15|            1|          1999.98|\n",
            "|2023-01-16|            1|          1999.99|\n",
            "|2023-01-17|            1|           179.97|\n",
            "|2023-01-18|            1|           899.99|\n",
            "|2023-01-19|            1|99.94999999999999|\n",
            "|2023-01-20|            1|           179.98|\n",
            "|2023-01-21|            1|            29.99|\n",
            "|2023-01-22|            1|           149.99|\n",
            "|2023-01-23|            1|           999.99|\n",
            "+----------+-------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 10 : Implémentation d'une Fonction de Fenêtre***"
      ],
      "metadata": {
        "id": "q8GKV6_EtEUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank, row_number, col\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"Exercice 10: Fonctions de Fenêtre\").getOrCreate()\n",
        "\n",
        "# Création d'un DataFrame d'employés\n",
        "employes_data = [\n",
        "    (\"Alice\", \"RH\", 55000),\n",
        "    (\"Bob\", \"Informatique\", 65000),\n",
        "    (\"Charlie\", \"Ventes\", 48000),\n",
        "    (\"David\", \"Informatique\", 70000),\n",
        "    (\"Eve\", \"RH\", 52000),\n",
        "    (\"Frank\", \"Ventes\", 49000),\n",
        "    (\"Grace\", \"Informatique\", 67000),\n",
        "    (\"Hannah\", \"Ventes\", 55000),\n",
        "    (\"Ian\", \"RH\", 60000),\n",
        "    (\"Julia\", \"Informatique\", 72000)\n",
        "]\n",
        "df_employes = spark.createDataFrame(employes_data, [\"Nom\", \"Departement\", \"Salaire\"])\n",
        "\n",
        "# Affichage du DataFrame original\n",
        "print(\"DataFrame des employés:\")\n",
        "df_employes.show()\n",
        "\n",
        "# Définition d'une fenêtre partitionnée par département et ordonnée par salaire décroissant\n",
        "window_spec = Window.partitionBy(\"Departement\").orderBy(col(\"Salaire\").desc())\n",
        "\n",
        "# Calcul du rang (rank) des employés par salaire au sein de chaque département\n",
        "df_avec_rank = df_employes.withColumn(\"Rang\", rank().over(window_spec))\n",
        "\n",
        "# Affichage du résultat avec le rang\n",
        "print(\"Classement des employés par salaire dans chaque département (rank):\")\n",
        "df_avec_rank.show()\n",
        "\n",
        "# Calcul du rang dense (dense_rank) des employés par salaire au sein de chaque département\n",
        "df_avec_dense_rank = df_employes.withColumn(\"Dense_Rang\", dense_rank().over(window_spec))\n",
        "\n",
        "# Affichage du résultat avec le rang dense\n",
        "print(\"Classement des employés par salaire dans chaque département (dense_rank):\")\n",
        "df_avec_dense_rank.show()\n",
        "\n",
        "# Calcul du numéro de ligne (row_number) des employés par salaire au sein de chaque département\n",
        "df_avec_row_number = df_employes.withColumn(\"Numero_Ligne\", row_number().over(window_spec))\n",
        "\n",
        "# Affichage du résultat avec le numéro de ligne\n",
        "print(\"Classement des employés par salaire dans chaque département (row_number):\")\n",
        "df_avec_row_number.show()\n",
        "\n",
        "# Combinaison des trois fonctions de fenêtre dans un seul DataFrame\n",
        "df_combine = df_employes \\\n",
        "    .withColumn(\"Rang\", rank().over(window_spec)) \\\n",
        "    .withColumn(\"Dense_Rang\", dense_rank().over(window_spec)) \\\n",
        "    .withColumn(\"Numero_Ligne\", row_number().over(window_spec))\n",
        "\n",
        "# Affichage du résultat combiné\n",
        "print(\"Tableau combiné avec les trois fonctions de fenêtre:\")\n",
        "df_combine.show()\n",
        "\n",
        "# Arrêt de la session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L341hQOttOqY",
        "outputId": "1f1b07dd-35ff-4320-f96f-27621840f94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame des employés:\n",
            "+-------+------------+-------+\n",
            "|    Nom| Departement|Salaire|\n",
            "+-------+------------+-------+\n",
            "|  Alice|          RH|  55000|\n",
            "|    Bob|Informatique|  65000|\n",
            "|Charlie|      Ventes|  48000|\n",
            "|  David|Informatique|  70000|\n",
            "|    Eve|          RH|  52000|\n",
            "|  Frank|      Ventes|  49000|\n",
            "|  Grace|Informatique|  67000|\n",
            "| Hannah|      Ventes|  55000|\n",
            "|    Ian|          RH|  60000|\n",
            "|  Julia|Informatique|  72000|\n",
            "+-------+------------+-------+\n",
            "\n",
            "Classement des employés par salaire dans chaque département (rank):\n",
            "+-------+------------+-------+----+\n",
            "|    Nom| Departement|Salaire|Rang|\n",
            "+-------+------------+-------+----+\n",
            "|  Julia|Informatique|  72000|   1|\n",
            "|  David|Informatique|  70000|   2|\n",
            "|  Grace|Informatique|  67000|   3|\n",
            "|    Bob|Informatique|  65000|   4|\n",
            "|    Ian|          RH|  60000|   1|\n",
            "|  Alice|          RH|  55000|   2|\n",
            "|    Eve|          RH|  52000|   3|\n",
            "| Hannah|      Ventes|  55000|   1|\n",
            "|  Frank|      Ventes|  49000|   2|\n",
            "|Charlie|      Ventes|  48000|   3|\n",
            "+-------+------------+-------+----+\n",
            "\n",
            "Classement des employés par salaire dans chaque département (dense_rank):\n",
            "+-------+------------+-------+----------+\n",
            "|    Nom| Departement|Salaire|Dense_Rang|\n",
            "+-------+------------+-------+----------+\n",
            "|  Julia|Informatique|  72000|         1|\n",
            "|  David|Informatique|  70000|         2|\n",
            "|  Grace|Informatique|  67000|         3|\n",
            "|    Bob|Informatique|  65000|         4|\n",
            "|    Ian|          RH|  60000|         1|\n",
            "|  Alice|          RH|  55000|         2|\n",
            "|    Eve|          RH|  52000|         3|\n",
            "| Hannah|      Ventes|  55000|         1|\n",
            "|  Frank|      Ventes|  49000|         2|\n",
            "|Charlie|      Ventes|  48000|         3|\n",
            "+-------+------------+-------+----------+\n",
            "\n",
            "Classement des employés par salaire dans chaque département (row_number):\n",
            "+-------+------------+-------+------------+\n",
            "|    Nom| Departement|Salaire|Numero_Ligne|\n",
            "+-------+------------+-------+------------+\n",
            "|  Julia|Informatique|  72000|           1|\n",
            "|  David|Informatique|  70000|           2|\n",
            "|  Grace|Informatique|  67000|           3|\n",
            "|    Bob|Informatique|  65000|           4|\n",
            "|    Ian|          RH|  60000|           1|\n",
            "|  Alice|          RH|  55000|           2|\n",
            "|    Eve|          RH|  52000|           3|\n",
            "| Hannah|      Ventes|  55000|           1|\n",
            "|  Frank|      Ventes|  49000|           2|\n",
            "|Charlie|      Ventes|  48000|           3|\n",
            "+-------+------------+-------+------------+\n",
            "\n",
            "Tableau combiné avec les trois fonctions de fenêtre:\n",
            "+-------+------------+-------+----+----------+------------+\n",
            "|    Nom| Departement|Salaire|Rang|Dense_Rang|Numero_Ligne|\n",
            "+-------+------------+-------+----+----------+------------+\n",
            "|  Julia|Informatique|  72000|   1|         1|           1|\n",
            "|  David|Informatique|  70000|   2|         2|           2|\n",
            "|  Grace|Informatique|  67000|   3|         3|           3|\n",
            "|    Bob|Informatique|  65000|   4|         4|           4|\n",
            "|    Ian|          RH|  60000|   1|         1|           1|\n",
            "|  Alice|          RH|  55000|   2|         2|           2|\n",
            "|    Eve|          RH|  52000|   3|         3|           3|\n",
            "| Hannah|      Ventes|  55000|   1|         1|           1|\n",
            "|  Frank|      Ventes|  49000|   2|         2|           2|\n",
            "|Charlie|      Ventes|  48000|   3|         3|           3|\n",
            "+-------+------------+-------+----+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Partie 2***"
      ],
      "metadata": {
        "id": "CLG8LwXTxMf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 1 : Mise en route avec PySpark***"
      ],
      "metadata": {
        "id": "VtEYli4NxY-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Fonction pour générer des données aléatoires\n",
        "def generate_random_data(n):\n",
        "    data = []\n",
        "    for _ in range(n):\n",
        "        # Générer une ligne de texte aléatoire\n",
        "        nom = ''.join(random.choices(string.ascii_letters, k=5))  # 5 caractères aléatoires\n",
        "        age = random.randint(18, 65)\n",
        "        ville = random.choice([\"Paris\", \"Londres\", \"New York\", \"Berlin\", \"Madrid\"])\n",
        "        ligne = f\"{nom},{age},{ville}\\n\"\n",
        "        data.append(ligne)\n",
        "    return data\n",
        "\n",
        "# Nombre de lignes à générer\n",
        "n = 100  # Générer 100 lignes\n",
        "\n",
        "# Générer les données\n",
        "data = generate_random_data(n)\n",
        "\n",
        "# Spécifier le chemin du fichier où les données seront enregistrées\n",
        "chemin_fichier = \"fichier_exemple.txt\"\n",
        "\n",
        "# Écrire les données dans un fichier texte\n",
        "with open(chemin_fichier, 'w') as f:\n",
        "    f.writelines(data)\n",
        "\n",
        "print(f\"Le fichier '{chemin_fichier}' a été généré avec {n} lignes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InPv-ZVAx3B-",
        "outputId": "d4899a8c-d304-42d0-852f-a62b077bed86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier 'fichier_exemple.txt' a été généré avec 100 lignes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Configuration de Spark\n",
        "conf = SparkConf().setAppName(\"Exercice Mise en Route\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Lecture du fichier texte\n",
        "chemin_fichier = \"fichier_exemple.txt\"\n",
        "rddStr = sc.textFile(chemin_fichier)\n",
        "\n",
        "# Afficher le contenu\n",
        "print(\"Contenu du fichier (10 premières lignes):\")\n",
        "for ligne in rddStr.take(10):\n",
        "    print(ligne)\n",
        "\n",
        "# Lecture du fichier texte\n",
        "chemin_fichieR = \"worldcitiespop.txt\"\n",
        "rddStr = sc.textFile(chemin_fichieR)\n",
        "\n",
        "print(\"\\nPremière 10 lignes de worldcitiespop.txt:\")\n",
        "for ligne in rddStr.take(10):\n",
        "    print(ligne)\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkL4E31TyA_f",
        "outputId": "e804e7a5-6938-4063-8c85-f8fd2410f6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenu du fichier (10 premières lignes):\n",
            "MtJtY,32,Berlin\n",
            "tnhGR,33,Londres\n",
            "iOnUF,63,New York\n",
            "VmKmT,56,Berlin\n",
            "psiWY,54,Londres\n",
            "hkyMJ,22,Berlin\n",
            "dKxXQ,53,Berlin\n",
            "zOJKw,28,Paris\n",
            "ZNiVw,35,Berlin\n",
            "ESNpK,23,Paris\n",
            "\n",
            "Première 10 lignes de worldcitiespop.txt:\n",
            "Country,City,AccentCity,Region,Population,Latitude,Longitude\n",
            "ad,aixas,Aix�s,06,,42.4833333,1.4666667\n",
            "ad,aixirivali,Aixirivali,06,,42.4666667,1.5\n",
            "ad,aixirivall,Aixirivall,06,,42.4666667,1.5\n",
            "ad,aixirvall,Aixirvall,06,,42.4666667,1.5\n",
            "ad,aixovall,Aixovall,06,,42.4666667,1.4833333\n",
            "ad,andorra,Andorra,07,,42.5,1.5166667\n",
            "ad,andorra la vella,Andorra la Vella,07,20430,42.5,1.5166667\n",
            "ad,andorra-vieille,Andorra-Vieille,07,,42.5,1.5166667\n",
            "ad,andorre,Andorre,07,,42.5,1.5166667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 2 : Nettoyage du fichier worldcitiespop.***"
      ],
      "metadata": {
        "id": "ZVMPcFWo0CYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import *\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"Nettoyage worldcitiespop\")\n",
        "\n",
        "# Chemin vers le fichier - à remplacer par votre chemin\n",
        "fichier_path = \"worldcitiespop.txt\"\n",
        "\n",
        "# Lecture du fichier\n",
        "rdd = sc.textFile(fichier_path)\n",
        "\n",
        "# Afficher la première ligne pour comprendre la structure\n",
        "premiere_ligne = rdd.first()\n",
        "print(\"Première ligne du fichier:\")\n",
        "print(premiere_ligne)\n",
        "\n",
        "# En examinant la première ligne, on s'aperçoit que les données sont séparées par des virgules\n",
        "# On transforme chaque ligne en liste d'éléments\n",
        "rdd_split = rdd.map(lambda line: line.split(','))\n",
        "\n",
        "# Conservation uniquement des lignes avec une population non vide (colonne 4)\n",
        "# La première ligne est souvent l'en-tête, donc on vérifie également que la colonne 4 est numérique\n",
        "def est_valide(elements):\n",
        "    try:\n",
        "        # Vérifie si la ligne a au moins 5 colonnes et si la population est un nombre\n",
        "        return len(elements) >= 5 and elements[4] and elements[4].isdigit()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "rdd_valide = rdd_split.filter(est_valide)\n",
        "\n",
        "# Comptage des lignes valides\n",
        "nb_lignes_valides = rdd_valide.count()\n",
        "print(f\"Nombre de lignes valides avec population: {nb_lignes_valides}\")\n",
        "\n",
        "# Affichage des 5 premières lignes valides\n",
        "print(\"5 premières lignes valides:\")\n",
        "for ligne in rdd_valide.take(5):\n",
        "    print(ligne)\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGbRpODb205I",
        "outputId": "02cb51e5-f50a-4a79-8878-0ee7c816cc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Première ligne du fichier:\n",
            "Country,City,AccentCity,Region,Population,Latitude,Longitude\n",
            "Nombre de lignes valides avec population: 47980\n",
            "5 premières lignes valides:\n",
            "['ad', 'andorra la vella', 'Andorra la Vella', '07', '20430', '42.5', '1.5166667']\n",
            "['ad', 'canillo', 'Canillo', '02', '3292', '42.5666667', '1.6']\n",
            "['ad', 'encamp', 'Encamp', '03', '11224', '42.5333333', '1.5833333']\n",
            "['ad', 'la massana', 'La Massana', '04', '7211', '42.55', '1.5166667']\n",
            "['ad', 'les escaldes', 'Les Escaldes', '08', '15854', '42.5', '1.5333333']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 3 : Statistiques***"
      ],
      "metadata": {
        "id": "QS19t43F3fI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import *\n",
        "import numpy as np\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"Statistiques worldcitiespop\")\n",
        "\n",
        "# Chemin vers le fichier\n",
        "fichier_path = \"worldcitiespop.txt\"\n",
        "\n",
        "# Lecture du fichier\n",
        "rdd = sc.textFile(fichier_path)\n",
        "\n",
        "# Transformation en liste et filtrage des lignes valides avec population\n",
        "def est_valide(elements):\n",
        "    try:\n",
        "        return len(elements) >= 5 and elements[4] and elements[4].isdigit()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "rdd_split = rdd.map(lambda line: line.split(','))\n",
        "rdd_valide = rdd_split.filter(est_valide)\n",
        "\n",
        "# Extraction des populations (conversion en float)\n",
        "rdd_populations = rdd_valide.map(lambda x: float(x[4]))\n",
        "\n",
        "# Calcul des statistiques\n",
        "min_pop = rdd_populations.min()\n",
        "max_pop = rdd_populations.max()\n",
        "sum_pop = rdd_populations.sum()\n",
        "count_pop = rdd_populations.count()\n",
        "avg_pop = sum_pop / count_pop\n",
        "\n",
        "# Calcul de l'écart-type\n",
        "# Pour calculer l'écart-type, nous avons besoin de la moyenne des carrés des différences\n",
        "# On calcule d'abord la somme des carrés des différences\n",
        "sum_squared_diff = rdd_populations.map(lambda x: (x - avg_pop) ** 2).sum()\n",
        "# Puis on divise par le nombre d'éléments pour obtenir la variance\n",
        "variance = sum_squared_diff / count_pop\n",
        "# L'écart-type est la racine carrée de la variance\n",
        "stdev_pop = sqrt(variance)\n",
        "\n",
        "print(f\"Statistiques sur les populations des villes:\")\n",
        "print(f\"Nombre de villes: {count_pop}\")\n",
        "print(f\"Population minimum: {min_pop}\")\n",
        "print(f\"Population maximum: {max_pop}\")\n",
        "print(f\"Somme des populations: {sum_pop}\")\n",
        "print(f\"Population moyenne: {avg_pop}\")\n",
        "print(f\"Écart-type des populations: {stdev_pop}\")\n",
        "\n",
        "# Format attendu dans l'exercice\n",
        "print(f\"(count: {count_pop}, mean: {avg_pop}, stdev: {stdev_pop}, max: {max_pop}, min: {min_pop})\")\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahNwJgss3gfI",
        "outputId": "6d0ed630-b2b2-4db6-a81c-29200b8cbd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistiques sur les populations des villes:\n",
            "Nombre de villes: 47980\n",
            "Population minimum: 7.0\n",
            "Population maximum: 31480498.0\n",
            "Somme des populations: 2289584999.0\n",
            "Population moyenne: 47719.57063359733\n",
            "Écart-type des populations: 302885.5592040371\n",
            "(count: 47980, mean: 47719.57063359733, stdev: 302885.5592040371, max: 31480498.0, min: 7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 4 : Histogrammes***"
      ],
      "metadata": {
        "id": "53YIFrpF30LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import *\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"Histogramme worldcitiespop\")\n",
        "\n",
        "# Chemin vers le fichier\n",
        "fichier_path = \"worldcitiespop.txt\"\n",
        "\n",
        "# Lecture du fichier\n",
        "rdd = sc.textFile(fichier_path)\n",
        "\n",
        "# Transformation en liste et filtrage des lignes valides avec population\n",
        "def est_valide(elements):\n",
        "    try:\n",
        "        return len(elements) >= 5 and elements[4] and elements[4].isdigit()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "rdd_split = rdd.map(lambda line: line.split(','))\n",
        "rdd_valide = rdd_split.filter(est_valide)\n",
        "\n",
        "# Extraction des populations (conversion en float)\n",
        "rdd_populations = rdd_valide.map(lambda x: float(x[4]))\n",
        "\n",
        "# Fonction pour déterminer la classe logarithmique\n",
        "def classe_log(population):\n",
        "    if population == 0:\n",
        "        return 0\n",
        "    return int(log10(population))\n",
        "\n",
        "# Calcul de l'histogramme (classe_log, count)\n",
        "rdd_histo = rdd_populations.map(lambda pop: (classe_log(pop), 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Tri de l'histogramme par classe\n",
        "histo_sorted = rdd_histo.sortByKey().collect()\n",
        "\n",
        "print(\"Histogramme des populations (échelle logarithmique):\")\n",
        "for classe, count in histo_sorted:\n",
        "    print(f\"Classe {classe} [10^{classe}..10^{classe+1}[: {count} villes\")\n",
        "\n",
        "# Format attendu dans l'exercice\n",
        "print(histo_sorted)\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfgYN1i36GU",
        "outputId": "3ac71410-4cbc-415b-b377-770750d4b6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Histogramme des populations (échelle logarithmique):\n",
            "Classe 0 [10^0..10^1[: 5 villes\n",
            "Classe 1 [10^1..10^2[: 174 villes\n",
            "Classe 2 [10^2..10^3[: 2187 villes\n",
            "Classe 3 [10^3..10^4[: 20537 villes\n",
            "Classe 4 [10^4..10^5[: 21550 villes\n",
            "Classe 5 [10^5..10^6[: 3248 villes\n",
            "Classe 6 [10^6..10^7[: 269 villes\n",
            "Classe 7 [10^7..10^8[: 10 villes\n",
            "[(0, 5), (1, 174), (2, 2187), (3, 20537), (4, 21550), (5, 3248), (6, 269), (7, 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 5 : TopK***"
      ],
      "metadata": {
        "id": "AvqE8m604Lmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import *\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"TopK worldcitiespop\")\n",
        "\n",
        "# Chemin vers le fichier\n",
        "fichier_path = \"worldcitiespop.txt\"\n",
        "\n",
        "# Lecture du fichier\n",
        "rdd = sc.textFile(fichier_path)\n",
        "\n",
        "# Transformation en liste et filtrage des lignes valides avec population\n",
        "def est_valide(elements):\n",
        "    try:\n",
        "        return len(elements) >= 5 and elements[4] and elements[4].isdigit()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "rdd_split = rdd.map(lambda line: line.split(','))\n",
        "rdd_valide = rdd_split.filter(est_valide)\n",
        "\n",
        "# Extraction des villes avec leurs populations\n",
        "# Format: (pays, ville asciiname, ville nom, région, population, latitude, longitude)\n",
        "rdd_villes = rdd_valide.map(lambda x: (x[0], x[1], x[2], x[3], float(x[4]), float(x[5]) if len(x) > 5 and x[5] else 0, float(x[6]) if len(x) > 6 and x[6] else 0))\n",
        "\n",
        "# Tri par population décroissante\n",
        "rdd_top_villes = rdd_villes.sortBy(lambda x: -x[4])\n",
        "\n",
        "# Récupération du top 10\n",
        "top10 = rdd_top_villes.take(10)\n",
        "\n",
        "print(\"Top 10 des villes les plus peuplées:\")\n",
        "for i, ville in enumerate(top10, 1):\n",
        "    print(f\"{i}. {ville[2]} ({ville[0]}): {int(ville[4])} habitants\")\n",
        "\n",
        "# Affichage au format demandé dans l'exercice\n",
        "print(\"\\nFormat demandé:\")\n",
        "for ville in top10:\n",
        "    print(f\"{ville[0]},{ville[1]},{ville[2]},{ville[3]},{int(ville[4])},{ville[5]},{ville[6]}\")\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iB1kOd44PiA",
        "outputId": "016d12e5-bbbb-4de8-c06e-a52c197b2764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 des villes les plus peuplées:\n",
            "1. Tokyo (jp): 31480498 habitants\n",
            "2. Shanghai (cn): 14608512 habitants\n",
            "3. Bombay (in): 12692717 habitants\n",
            "4. Karachi (pk): 11627378 habitants\n",
            "5. Delhi (in): 10928270 habitants\n",
            "6. New Delhi (in): 10928270 habitants\n",
            "7. Manila (ph): 10443877 habitants\n",
            "8. Moscow (ru): 10381288 habitants\n",
            "9. Seoul (kr): 10323448 habitants\n",
            "10. S�o Paulo (br): 10021437 habitants\n",
            "\n",
            "Format demandé:\n",
            "jp,tokyo,Tokyo,40,31480498,35.685,139.751389\n",
            "cn,shanghai,Shanghai,23,14608512,31.045556,121.399722\n",
            "in,bombay,Bombay,16,12692717,18.975,72.825833\n",
            "pk,karachi,Karachi,05,11627378,24.9056,67.0822\n",
            "in,delhi,Delhi,07,10928270,28.666667,77.216667\n",
            "in,new delhi,New Delhi,07,10928270,28.6,77.2\n",
            "ph,manila,Manila,D9,10443877,14.6042,120.9822\n",
            "ru,moscow,Moscow,48,10381288,55.752222,37.615556\n",
            "kr,seoul,Seoul,11,10323448,37.5985,126.9783\n",
            "br,sao paulo,S�o Paulo,27,10021437,-23.473293,-46.665803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Exercice 6 : Re-cleaning***"
      ],
      "metadata": {
        "id": "OIjgHC4s4Y_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import *\n",
        "\n",
        "# Initialisation du contexte Spark\n",
        "sc = SparkContext(\"local\", \"Re-cleaning worldcitiespop\")\n",
        "\n",
        "# Chemin vers le fichier\n",
        "fichier_path = \"worldcitiespop.txt\"\n",
        "\n",
        "# Lecture du fichier\n",
        "rdd = sc.textFile(fichier_path)\n",
        "\n",
        "# Transformation en liste et filtrage des lignes valides avec population\n",
        "def est_valide(elements):\n",
        "    try:\n",
        "        return len(elements) >= 5 and elements[4] and elements[4].isdigit()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "rdd_split = rdd.map(lambda line: line.split(','))\n",
        "rdd_valide = rdd_split.filter(est_valide)\n",
        "\n",
        "# Extraction des villes avec leurs populations\n",
        "# Format: ((latitude, longitude), (pays, ville asciiname, ville nom, région, population))\n",
        "# On arrondit les coordonnées à 2 décimales pour regrouper les villes proches\n",
        "def extraire_ville(x):\n",
        "    lat = round(float(x[5]), 2) if len(x) > 5 and x[5] else 0\n",
        "    lon = round(float(x[6]), 2) if len(x) > 6 and x[6] else 0\n",
        "    return ((lat, lon), (x[0], x[1], x[2], x[3], float(x[4]), float(x[5]) if len(x) > 5 and x[5] else 0, float(x[6]) if len(x) > 6 and x[6] else 0))\n",
        "\n",
        "rdd_villes = rdd_valide.map(extraire_ville)\n",
        "\n",
        "# Élimination des doublons en gardant les villes avec la plus grande population\n",
        "rdd_villes_unique = rdd_villes.reduceByKey(lambda a, b: a if a[4] > b[4] else b)\n",
        "\n",
        "# Convertir à nouveau en format plat pour les stats\n",
        "rdd_villes_clean = rdd_villes_unique.map(lambda x: x[1])\n",
        "\n",
        "# Extraction des populations pour les statistiques\n",
        "rdd_populations = rdd_villes_clean.map(lambda x: x[4])\n",
        "\n",
        "# Calcul des statistiques\n",
        "min_pop = rdd_populations.min()\n",
        "max_pop = rdd_populations.max()\n",
        "sum_pop = rdd_populations.sum()\n",
        "count_pop = rdd_populations.count()\n",
        "avg_pop = sum_pop / count_pop\n",
        "\n",
        "# Calcul de l'écart-type\n",
        "sum_squared_diff = rdd_populations.map(lambda x: (x - avg_pop) ** 2).sum()\n",
        "variance = sum_squared_diff / count_pop\n",
        "stdev_pop = sqrt(variance)\n",
        "\n",
        "print(f\"Statistiques après élimination des doublons:\")\n",
        "print(f\"(count: {count_pop}, mean: {avg_pop}, stdev: {stdev_pop}, max: {max_pop}, min: {min_pop})\")\n",
        "\n",
        "# Calcul de l'histogramme\n",
        "def classe_log(population):\n",
        "    if population == 0:\n",
        "        return 0\n",
        "    return int(log10(population))\n",
        "\n",
        "rdd_histo = rdd_populations.map(lambda pop: (classe_log(pop), 1)).reduceByKey(lambda a, b: a + b)\n",
        "histo_sorted = rdd_histo.sortByKey().collect()\n",
        "print(histo_sorted)\n",
        "\n",
        "# Top 20 des villes les plus peuplées après élimination des doublons\n",
        "top20 = rdd_villes_clean.sortBy(lambda x: -x[4]).take(20)\n",
        "\n",
        "# Affichage au format demandé\n",
        "for ville in top20:\n",
        "    print(f\"{ville[0]},{ville[1]},{ville[2]},{ville[3]},{int(ville[4])},{ville[5]},{ville[6]}\")\n",
        "\n",
        "# Arrêt du contexte Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZsFL5Fu4oN0",
        "outputId": "e3ec9b5e-52d3-4b7c-ce17-9d69533bda67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistiques après élimination des doublons:\n",
            "(count: 47792, mean: 47763.707356879815, stdev: 303310.60189777106, max: 31480498.0, min: 7.0)\n",
            "[(0, 5), (1, 167), (2, 2144), (3, 20446), (4, 21513), (5, 3240), (6, 267), (7, 10)]\n",
            "jp,tokyo,Tokyo,40,31480498,35.685,139.751389\n",
            "cn,shanghai,Shanghai,23,14608512,31.045556,121.399722\n",
            "in,bombay,Bombay,16,12692717,18.975,72.825833\n",
            "pk,karachi,Karachi,05,11627378,24.9056,67.0822\n",
            "in,delhi,Delhi,07,10928270,28.666667,77.216667\n",
            "in,new delhi,New Delhi,07,10928270,28.6,77.2\n",
            "ph,manila,Manila,D9,10443877,14.6042,120.9822\n",
            "ru,moscow,Moscow,48,10381288,55.752222,37.615556\n",
            "kr,seoul,Seoul,11,10323448,37.5985,126.9783\n",
            "br,sao paulo,S�o Paulo,27,10021437,-23.473293,-46.665803\n",
            "tr,istanbul,Istanbul,34,9797536,41.018611,28.964722\n",
            "ng,lagos,Lagos,05,8789133,6.453056,3.395833\n",
            "mx,mexico,Mexico,09,8720916,19.434167,-99.138611\n",
            "id,jakarta,Jakarta,04,8540306,-6.174444,106.829444\n",
            "us,new york,New York,NY,8107916,40.7141667,-74.0063889\n",
            "cd,kinshasa,Kinshasa,06,7787832,-4.3,15.3\n",
            "eg,cairo,Cairo,11,7734602,30.05,31.25\n",
            "pe,lima,Lima,15,7646786,-12.05,-77.05\n",
            "cn,peking,Peking,22,7480601,39.928889,116.388333\n",
            "gb,london,London,H9,7421228,51.514125,-0.093689\n"
          ]
        }
      ]
    }
  ]
}